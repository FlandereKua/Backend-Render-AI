import google.generativeai as genai
from google.generativeai.protos import Part
from google.generativeai.types import StopCandidateException
import re
import asyncio
import json
from pathlib import Path
from PIL import Image
from app.core.config import GEMINI_API_KEY, MODEL_PRO, MODEL_FLASH, SYSTEM_PROMPT_V7
from app.models.schemas import ThinkingChunk, ThinkingDone, FinalAnswer, ErrorMessage, StatusUpdate
from app.services.tool_executor import available_tools, tool_status_messages
from app.db import history_manager

genai.configure(api_key=GEMINI_API_KEY)

def sanitize_and_format_for_html(text: str) -> str:
    cleaned_text = text.replace('**', '').replace('###', '').replace('##', '').replace('#', '')
    formatted_text = re.sub(r'`([^`]+)`', r'<code>\1</code>', cleaned_text)
    return formatted_text

async def process_user_request(
    prompt: str,
    session_id: str,
    image_bytes: bytes | None = None,
    file_content: str | None = None,
    filename: str | None = None,
    mime_type: str | None = None
):
    retrieved_history = history_manager.get_history(session_id, limit=10)
    user_message = prompt
    if filename:
        user_message += f"\n(File ƒë√≠nh k√®m: {filename})"
    history_manager.add_message(session_id, "user", user_message)

    decision = ""
    if file_content or image_bytes:
        decision = "complex_reasoning"
    else:
        router_model = genai.GenerativeModel(MODEL_FLASH)
        router_prompt = f"""
        Analyze the user's prompt and classify it into one of two categories:
        1. 'simple_answer': For general knowledge questions, greetings, or topics that do not require real-time information.
        2. 'complex_reasoning': For questions about recent events, specific products, companies, or anything that requires up-to-date information or deep analysis.
        User Prompt: "{prompt}"
        Respond with ONLY 'simple_answer' or 'complex_reasoning'.
        """
        try:
            router_response = await router_model.generate_content_async(router_prompt)
            decision = router_response.text.strip()
        except Exception:
            decision = "complex_reasoning"

    final_model_answer = ""
    try:
        if decision == 'simple_answer':
            simple_model = genai.GenerativeModel(MODEL_FLASH)
            chat_session = simple_model.start_chat(history=retrieved_history)
            
            if not retrieved_history:
                persona_prefix = """
                B·∫°n l√† Locaith AI, m·ªôt tr·ª£ l√Ω ·∫£o th√¢n thi·ªán v√† chuy√™n nghi·ªáp t·ª´ Locaith Solution Tech.
                Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
                1. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng, gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc n·∫øu c·∫ßn.
                2. Sau khi tr·∫£ l·ªùi xong, h√£y ƒë·ªÅ xu·∫•t 2-3 c√¢u h·ªèi ti·∫øp theo m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ quan t√¢m, d·ª±a tr√™n ng·ªØ c·∫£nh cu·ªôc h·ªôi tho·∫°i.
                """
                simple_prompt = f"{persona_prefix}\n\nC√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: \"{prompt}\""
            else:
                persona_prefix = """
                Ti·∫øp t·ª•c cu·ªôc h·ªôi tho·∫°i v·ªõi vai tr√≤ l√† Locaith AI.
                Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
                1. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng, d·ª±a tr√™n ng·ªØ c·∫£nh ƒë√£ c√≥.
                2. Sau khi tr·∫£ l·ªùi xong, h√£y ƒë·ªÅ xu·∫•t 2-3 c√¢u h·ªèi ti·∫øp theo m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ quan t√¢m.
                """
                simple_prompt = f"{persona_prefix}\n\nC√¢u h·ªèi ti·∫øp theo c·ªßa ng∆∞·ªùi d√πng: \"{prompt}\""

            response = await chat_session.send_message_async(simple_prompt)
            final_model_answer = response.text
            final_answer_obj = FinalAnswer(content=final_model_answer)
            yield f"data: {final_answer_obj.model_dump_json()}\n\n"
        
        else:
            full_thinking_process = []
            
            if image_bytes and mime_type:
                yield f"data: {StatusUpdate(content='üëÅÔ∏è ƒêang ph√¢n t√≠ch h√¨nh ·∫£nh b·∫±ng `gemini-2.5-pro`...').model_dump_json()}\n\n"
                
                vision_model = genai.GenerativeModel(MODEL_PRO)
                
                image_part = Part(inline_data={'mime_type': mime_type, 'data': image_bytes})
                
                prompt_part = f"""
                V·ªõi vai tr√≤ l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, h√£y th·ª±c hi·ªán m·ªôt b√†i ph√¢n t√≠ch chi ti·∫øt v·ªÅ h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.
                QUAN TR·ªåNG: To√†n b·ªô b√†i ph√¢n t√≠ch chi ti·∫øt c·ªßa b·∫°n PH·∫¢I ƒë∆∞·ª£c ƒë·∫∑t trong c·∫∑p th·∫ª `<thinking>` v√† `</thinking>`.
                Y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng: "{prompt}"
                """

                response = await vision_model.generate_content_async([prompt_part, image_part], stream=True)
                
                async for chunk in response:
                    if chunk.text:
                        sanitized_content = sanitize_and_format_for_html(chunk.text)
                        yield f"data: {ThinkingChunk(content=sanitized_content).model_dump_json()}\n\n"
                        full_thinking_process.append(sanitized_content)

            else:
                model_pro = genai.GenerativeModel(MODEL_PRO)
                chat_session = model_pro.start_chat(history=retrieved_history)
                
                prompt_for_thinking = f"{SYSTEM_PROMPT_V7}\n\n## USER REQUEST ##\n{prompt}"
                if file_content:
                    prompt_for_thinking += f"\n\n## ATTACHED FILE CONTENT: `{filename}` ##\n---\n{file_content}\n---"
                
                response_stream = await chat_session.send_message_async(prompt_for_thinking, stream=True)
                
                current_chunk_buffer = ""
                async for chunk in response_stream:
                    if not chunk.text: continue
                    
                    sanitized_content = chunk.text.replace('**', '').replace('###', '').replace('##', '').replace('#', '')
                    yield f"data: {ThinkingChunk(content=sanitized_content).model_dump_json()}\n\n"
                    current_chunk_buffer += sanitized_content
                
                full_thinking_process.append(current_chunk_buffer)
                final_thinking_text_pass1 = current_chunk_buffer
                
                tool_call_match = re.search(r'\[CallTool: (\w+)\(query="((?:[^"\\]|\\.)*)"\)\]', final_thinking_text_pass1)

                if tool_call_match:
                    tool_name = tool_call_match.group(1)
                    tool_query = tool_call_match.group(2)
                    
                    if tool_name in available_tools:
                        status_message = tool_status_messages.get(tool_name, f"‚öôÔ∏è ƒêang th·ª±c thi c√¥ng c·ª• {tool_name}...")
                        yield f"data: {StatusUpdate(content=status_message).model_dump_json()}\n\n"

                        tool_function = available_tools[tool_name]
                        tool_result = await tool_function(tool_query)
                        
                        observation_prompt = f"Observation: {tool_result}"
                        full_thinking_process.append(f"\n[Observation from {tool_name}: Received structured data]\n{tool_result}\n")
                        
                        follow_up_stream = await chat_session.send_message_async(observation_prompt, stream=True)
                        async for follow_up_chunk in follow_up_stream:
                            if follow_up_chunk.text:
                                sanitized_content_after_tool = follow_up_chunk.text.replace('**', '').replace('###', '').replace('##', '').replace('#', '')
                                yield f"data: {ThinkingChunk(content=sanitized_content_after_tool).model_dump_json()}\n\n"
                                full_thinking_process.append(sanitized_content_after_tool)
            
            yield f"data: {ThinkingDone().model_dump_json()}\n\n"
            
            synthesizer_model = genai.GenerativeModel(MODEL_FLASH)
            final_thinking_text = "".join(full_thinking_process)
            
            prompt_for_synthesis = ""
            observation_match = re.search(r"\[Observation from \w+: Received structured data\]\n(\{.*\}|\[.*\])", final_thinking_text, re.DOTALL)
            
            if observation_match:
                observation_data_raw = observation_match.group(1).strip()
                prompt_for_synthesis = f"""
                Nhi·ªám v·ª• c·ªßa b·∫°n l√† m·ªôt chuy√™n gia tr√¨nh b√†y d·ªØ li·ªáu. D·ª±a v√†o d·ªØ li·ªáu JSON th√¥ sau, h√£y ƒë·ªãnh d·∫°ng th√†nh danh s√°ch r√µ r√†ng cho ng∆∞·ªùi d√πng, tu√¢n th·ªß c√°c quy t·∫Øc ƒë√£ bi·∫øt.
                D·ªØ li·ªáu JSON th√¥: --- {observation_data_raw} ---
                So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng:
                """
            else:
                raw_answer = final_thinking_text.split("</thinking>")[-1].strip()
                if not raw_answer: 
                    raw_answer = final_thinking_text
                
                if image_bytes:
                     prompt_for_synthesis = f"""
                    Nhi·ªám v·ª• c·ªßa b·∫°n l√† m·ªôt chuy√™n gia giao ti·∫øp. D·ª±a tr√™n lu·ªìng ph√¢n t√≠ch h√¨nh ·∫£nh chi ti·∫øt sau ƒë√¢y, h√£y vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p, th√¢n thi·ªán v√† chuy√™n nghi·ªáp cho ng∆∞·ªùi d√πng.
                    Kh√¥ng l·∫∑p l·∫°i t·∫•t c·∫£ chi ti·∫øt, ch·ªâ c·∫ßn t√≥m t·∫Øt c√°c ƒëi·ªÉm ch√≠nh m·ªôt c√°ch d·ªÖ hi·ªÉu v√† ƒë∆∞a ra c√°c g·ª£i √Ω ti·∫øp theo.
                    N·ªôi dung ph√¢n t√≠ch th√¥: --- {raw_answer} ---
                    So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng cho ng∆∞·ªùi d√πng:
                    """
                else:
                    prompt_for_synthesis = f"""
                    Nhi·ªám v·ª• c·ªßa b·∫°n l√† m·ªôt chuy√™n gia bi√™n t·∫≠p. D·ª±a tr√™n n·ªôi dung h·ªôi tho·∫°i th√¥ sau, h√£y bi√™n t·∫≠p l·∫°i n√≥ th√†nh c√¢u tr·∫£ l·ªùi cu·ªëi c√πng, ho√†n ch·ªânh v√† chuy√™n nghi·ªáp.
                    Gi·ªØ nguy√™n √Ω ch√≠nh, c·∫£i thi·ªán vƒÉn phong v√† ƒë·∫£m b·∫£o c√≥ g·ª£i √Ω c√¢u h·ªèi ti·∫øp theo.
                    N·ªôi dung th√¥ c·∫ßn bi√™n t·∫≠p: --- {raw_answer} ---
                    So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c ho√†n thi·ªán:
                    """
            
            synthesis_response = await synthesizer_model.generate_content_async(prompt_for_synthesis)
            final_model_answer = synthesis_response.text
            final_answer_obj = FinalAnswer(content=final_model_answer)
            yield f"data: {final_answer_obj.model_dump_json()}\n\n"

    except StopCandidateException as e:
        error_message = ErrorMessage(content="Y√™u c·∫ßu c·ªßa b·∫°n c√≥ th·ªÉ ch·ª©a n·ªôi dung kh√¥ng ph√π h·ª£p ho·∫∑c nh·∫°y c·∫£m. Vui l√≤ng th·ª≠ l·∫°i v·ªõi m·ªôt c√¢u h·ªèi kh√°c.")
        yield f"data: {error_message.model_dump_json()}\n\n"
    except Exception as e:
        error_message = ErrorMessage(content=f"ƒê√£ x·∫£y ra m·ªôt l·ªói n·ªôi b·ªô: {str(e)}")
        yield f"data: {error_message.model_dump_json()}\n\n"

    if final_model_answer:
        history_manager.add_message(session_id, "model", final_model_answer)